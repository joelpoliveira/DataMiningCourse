{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Home Assignment\n",
    "Made by: <br>\n",
    "- Diogo Araújo, fc60997 - 2 H <br>\n",
    "- Joel Oliveira, fc59442 - 3 H <br>\n",
    "- João Braz, f60419 - 2 H <br>\n",
    "\n",
    "For this first home assignment, we have been tasked with builing a few supervised learning models. The data we will be using was taken from the \"UCI Supercoductivity Data\" dataset, which is represented via the \"train.csv\" files and the \"unique_m.csv\" files. <br>\n",
    "\n",
    "The dependent variable (y) for the will be the last column of our data, which is the \"critical_temp\" column. <br>\n",
    "\n",
    "## Import Libraries and Data\n",
    "In this initial section, we are only going to be importing the libraries we will need for the coming objectives as well as obtaining the data from the \".csv\" files. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from scipy.sparse import dok_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, max_error, ConfusionMatrixDisplay, confusion_matrix, matthews_corrcoef\n",
    "\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore');\n",
    "\n",
    "# Get the data from the .csv files\n",
    "df_train = pd.read_csv('train.csv') #PCA -> dense\n",
    "df_unique = pd.read_csv('unique_m.csv') #SVD -> sparse\n",
    "\n",
    "# Merge both of the datasets\n",
    "df = df_train.merge(df_unique, left_index=True, right_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 1\n",
    "In this objective, of the first home assignment, we aim to create the dimensionality reduction model four our data. For this, we will be utilizing PCA (Principal Component Analysis) and then analyzing its results. <br>\n",
    "\n",
    "We will be utilizing the StandardScaler for scaling and PCA for reducing our data. However, this change will only happen during the model fitting in order to make it easier to compare our models, as such in this section the data will continue to not have been transformed in order to not alter this process. As such, in this section we will only be creating our Scaling and Dimensionality Reduction models. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Missing Values: 0\n",
      "Percentage of 0's in the X matrix = 0.495\n",
      "Number of components to reach 90% explainability with scaled dataset = 2\n"
     ]
    }
   ],
   "source": [
    "# Check the dataframe for missing values (doesnt appear to have any)\n",
    "print(\"Dataframe Missing Values:\", df.isna().sum().sum())\n",
    "\n",
    "# Get the X and Y from the dataframe\n",
    "x = df.drop(columns=[\"critical_temp_x\", \"critical_temp_y\", \"material\"])\n",
    "y = df.critical_temp_x\n",
    "\n",
    "# Split the data into training and testing sets (Full data and Dimensionality Reduction data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=50)\n",
    "\n",
    "# Check if merge of both dataframes is sparse or dense (on training set)\n",
    "print(f\"Percentage of 0's in the X matrix = {(x_train==0).sum().sum() / np.prod(x_train.shape):.3f}\")\n",
    "\n",
    "# Create the dimensionality reduction model (will be using PCA as df is dense, also n_components is set to 0.90 to get variance atleast above 90%)\n",
    "pca = PCA(n_components=0.90, svd_solver=\"full\")\n",
    "\n",
    "# Fit the model with the training data\n",
    "pca.fit(x_train)\n",
    "\n",
    "# Check model for number of components to reach explainability\n",
    "print(f\"Number of components to reach 90% explainability with scaled dataset = {len(pca.explained_variance_)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss Results (1)\n",
    "We start this exercise by checking the model for any missing values. As we found none, we instead passed onto splitting the data and then scaling it. We only scaled the training data as we were not going to change the testing data. <br>\n",
    "\n",
    "After this, as can be seen from the results obtained above, we checked for the percentage of values equal to 0 in our merged dataframe. From these, we determined that our data will be considered a dense matrix (as most of its values are not equal to 0) and, as such, we decided to use the PCA dimensionality recuction model as it is more effective in dense matrixes than SVD. From this, we then scale our data (using the \"with_median\" hyperparameter equal to false in order to not mess with the values equal to 0) and perform the dimensionality reduction (using PCA) on it. For the model, we only accepted that which allowed us to have a variance above 90%. <br>\n",
    "\n",
    "We then saw how many components we reached in order to reach explainability, which equalled to 66. This is a good decrease in the number of features, as originally there were 170. <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 2\n",
    "In this objective, we have to create the regression and classification models. <br>\n",
    "\n",
    "### 2.1)\n",
    "For this part, we are making the regression model. We will be using Decision Trees (DT), with their sklearn implementation, as PCA components are decorrolated between themselves. As such, with no linear relation between components we don't expect good performance from the Linear Regression (LR) model. <br>\n",
    "\n",
    "The complexity in this exercise will come from training the Decision Tree model. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time without PCA = 3.163\n",
      "Train time with PCA = 3.456\n",
      "Full dataset number of elements =  3195712 \n",
      "Reduced dataset number of elements =  1262976\n",
      "Prediction time without PCA = 0.020\n",
      "Prediction time with PCA = 0.023\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Full Dataset RMSE = 11.774 \t\t\t\t\t | Reduced Dataset RMSE = 12.106\n",
      "Full Dataset Max. Error = 114.400 \t\t\t\t | Reduced Dataset Max. Error = 87.400\n",
      "Full Dataset Pearson Corr. = 0.941 \t\t\t\t | Reduced Dataset Pearson Corr = 0.938\n"
     ]
    }
   ],
   "source": [
    "# Create (and test the time) for the Regression Models (for full and reduced data)\n",
    "t = time()\n",
    "dtr = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"regressor\", DecisionTreeRegressor())\n",
    "]).fit(x_train, y_train)\n",
    "print(f\"Train time without PCA = {time() - t:.3f}\")\n",
    "\n",
    "t = time()\n",
    "dtr_pca = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"dim\", PCA(n_components=0.9, svd_solver=\"full\")),\n",
    "    (\"regressor\", DecisionTreeRegressor())\n",
    "]).fit(x_train, y_train)\n",
    "print(f\"Train time with PCA = {time() - t:.3f}\")\n",
    "\n",
    "# Print out the number of elements in each data (check reduction)\n",
    "print(\"Full dataset number of elements = \", np.prod(x_train.shape), \n",
    "      \"\\nReduced dataset number of elements = \", np.prod(PCA(n_components=0.9, svd_solver=\"full\").fit_transform(\n",
    "            StandardScaler().fit_transform(x_train)\n",
    "        ).shape\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Get predictions for model\n",
    "t = time()\n",
    "preds = dtr.predict(x_test)\n",
    "print(f\"Prediction time without PCA = {time() - t:.3f}\")\n",
    "\n",
    "t=time()\n",
    "preds_pca = dtr_pca.predict(x_test)\n",
    "print(f\"Prediction time with PCA = {time() - t:.3f}\", end=\"\\n\"+\"-\"*120+\"\\n\")\n",
    "\n",
    "# Compute the Evaluation values (RMSE, max and pearson for full and dim. reduced data)\n",
    "rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "max_err = max_error(y_test, preds)\n",
    "pearson_r = np.corrcoef(y_test, preds)[0,1]\n",
    "\n",
    "rmse_pca = mean_squared_error(y_test, preds_pca, squared=False) \n",
    "max_err_pca = max_error(y_test, preds_pca)\n",
    "pearson_r_pca = np.corrcoef(y_test, preds_pca)[0,1]\n",
    "\n",
    "# Print out the evaluation values for the data (comparison)\n",
    "print(f\"Full Dataset RMSE = {rmse:.3f}\", \"\\t\"*5,  f\"| Reduced Dataset RMSE = {rmse_pca:.3f}\")\n",
    "print(f\"Full Dataset Max. Error = {max_err:.3f}\", \"\\t\"*4, f\"| Reduced Dataset Max. Error = {max_err_pca:.3f}\")\n",
    "print(f\"Full Dataset Pearson Corr. = {pearson_r:.3f}\", \"\\t\"*4, f\"| Reduced Dataset Pearson Corr = {pearson_r_pca:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss Results (2.1)\n",
    "As can be seen from the previous results, we can compare / evaluate the various results while using the full dataset and the dimensionality reduction dataset. <br>\n",
    "\n",
    "From the initial lines, we can see the training time between the two models is quite similar, so the complexity wont come from this part. The same can be said for the predicion time. Although this would increase with alot more elements, and as such the difference between these two would increase, it is not something to take into account for our case. <br>\n",
    "\n",
    "The total size between the data is also vastly changed, from the larger 3195712 elements of the full data compared to the 1262976 elements of the smaller, reduced data.<br>\n",
    "\n",
    "The regression difference is also minimal between the full or reduced datasets. While the RMSE stayed nearly the same, the maximum error obtained in the reduced dataset is reduced. <br>\n",
    "\n",
    "With these results, it is much better to use the reduced dataset as it they are extremely similar but it is smaller. <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2)\n",
    "For this part, we are making the classification model. From the exercise, we were instructed to use both NaiveBayes (NB) and DecisionTrees (DT) for the full and reduced datasets. <br>\n",
    "\n",
    "We will also have to create a few additional classes for the dependent variable. These will be VeryLow (0.0, 1.0), Low (0.0, 1.0), Medium (0.0, 1.0), High (0.0, 1.0) and VeryHigh (0.0, 1.0). We will then create two models, one with the full dataset (direct variables) and the other with the projection of the full data in a smaller data space (dimensionality reduction). These results will then be compared and discussed. <br>\n",
    "\n",
    "Our first objective, for this exercise, was to compare the models on the full dataset. After that, we would compare both of the models using the reduced dataset. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------FULL DATASET------------\n",
      "Decision Tree train time (no PCA) = 5.075\n",
      "Naive Bayes train time (no PCA) = 0.176\n",
      "Decision Tree predict time (no PCA) = 0.010\n",
      "Naive Bayes predict time (no PCA) = 0.019\n",
      "Decision Tree MFCC = 0.803 \t\t\t\t\t | Naive Bayes MFCC = 0.325\n",
      "-----------REDUCED DATASET------------\n",
      "Decision Tree train time (with PCA) = 4.501\n",
      "Naive Bayes train time (with PCA) = 0.809\n",
      "Decision Tree predict time (with PCA) = 0.013\n",
      "Naive Bayes predict time (with PCA) = 0.033\n",
      "Decision Tree MFCC = 0.783 \t\t\t\t\t | Naive Bayes MFCC = 0.195\n"
     ]
    }
   ],
   "source": [
    "# Create a function to add classes\n",
    "def to_class(x: float) -> str:\n",
    "    if 0 <= x < 1.0:\n",
    "        return \"VeryLow\"\n",
    "    elif 1 <= x < 5.0:\n",
    "        return \"Low\"\n",
    "    elif 5 <= x < 20.0:\n",
    "        return \"Medium\"\n",
    "    elif 20 <= x < 100.0:\n",
    "        return \"High\"\n",
    "    elif x >= 100:\n",
    "        return \"VeryHigh\"\n",
    "    return np.nan\n",
    "\n",
    "# Add classes to our dependent variable (apply function)\n",
    "y_train_class = y_train.apply(to_class)\n",
    "y_test_class = y_test.apply(to_class)\n",
    "\n",
    "## FULL DATA (not dim. reduced)\n",
    "print(\"-----------FULL DATASET------------\")\n",
    "\n",
    "# Create (and compare training time) for both models \n",
    "t = time()\n",
    "dtc = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"classifier\", DecisionTreeClassifier())\n",
    "]).fit(x_train, y_train_class)\n",
    "print(f\"Decision Tree train time (no PCA) = {time() - t:.3f}\")\n",
    "\n",
    "t = time()\n",
    "nb = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"classifier\", GaussianNB())\n",
    "]).fit(x_train, y_train_class)\n",
    "print(f\"Naive Bayes train time (no PCA) = {time() - t:.3f}\")\n",
    "\n",
    "# Get predictions from models (and check time)\n",
    "t = time()\n",
    "dtc_preds = dtc.predict(x_test)\n",
    "print(f\"Decision Tree predict time (no PCA) = {time() - t:.3f}\")\n",
    "\n",
    "t=time()\n",
    "nb_preds = nb.predict(x_test)\n",
    "print(f\"Naive Bayes predict time (no PCA) = {time() - t:.3f}\")\n",
    "\n",
    "# Compute model evaluations\n",
    "dtc_mfcc = matthews_corrcoef(y_test_class,dtc_preds)\n",
    "nb_mfcc= matthews_corrcoef(y_test_class, nb_preds)\n",
    "\n",
    "# Print out model evaluations (comparison)\n",
    "print(f\"Decision Tree MFCC = {dtc_mfcc:.3f}\", \"\\t\"*5,  f\"| Naive Bayes MFCC = {nb_mfcc:.3f}\")\n",
    "\n",
    "## REDUCED DATA (PCA reduced)\n",
    "print(\"-----------REDUCED DATASET------------\")\n",
    "\n",
    "# Create (and compare training time) for both models (also fit)\n",
    "t = time()\n",
    "dtc = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"dim\", PCA(n_components=0.9, svd_solver=\"full\")),\n",
    "    (\"classifier\", DecisionTreeClassifier())\n",
    "]).fit(x_train, y_train_class)\n",
    "print(f\"Decision Tree train time (with PCA) = {time() - t:.3f}\")\n",
    "\n",
    "t = time()\n",
    "nb = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"dim\", PCA(n_components=0.9, svd_solver=\"full\")),\n",
    "    (\"classifier\", GaussianNB())\n",
    "]).fit(x_train, y_train_class)\n",
    "print(f\"Naive Bayes train time (with PCA) = {time() - t:.3f}\")\n",
    "\n",
    "# Get predictions from models (and check time)\n",
    "t = time()\n",
    "dtc_preds = dtc.predict(x_test)\n",
    "print(f\"Decision Tree predict time (with PCA) = {time() - t:.3f}\")\n",
    "\n",
    "t = time()\n",
    "nb_preds = nb.predict(x_test)\n",
    "print(f\"Naive Bayes predict time (with PCA) = {time() - t:.3f}\")\n",
    "\n",
    "# Compute model evaluations\n",
    "dtc_mfcc = matthews_corrcoef(y_test_class, dtc_preds)\n",
    "nb_mfcc= matthews_corrcoef(y_test_class, nb_preds)\n",
    "\n",
    "# Print out model evaluations (comparison)\n",
    "print(f\"Decision Tree MFCC = {dtc_mfcc:.3f}\", \"\\t\"*5,  f\"| Naive Bayes MFCC = {nb_mfcc:.3f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss Results (2.2)\n",
    "Starting from the full set, we can check the first line. From this, we can tell how big of the difference in the training time between our models is. Given the small size of the data, this is an extremely noticeable difference as it will only increase more with size. <br>\n",
    "\n",
    "However, from the next lines we can tell that the DT model is actually faster in making the predictions than the NB model. This is due to it only parsing the tree, meaning a lower complexity, which is faster than calculating probabilities / likelihoods. The DT model also obtained a better MCFF score. <br>\n",
    "\n",
    "Moving on to the reduced set (which is after the evaluation line) we can see that the time they took to train was nearly the same, although it had a bigger impact in the NB model which took a significant more time. <br>\n",
    "\n",
    "Een with the independence of the variables, the NB model did not have a good performance. This is most likely due to the DT model having universal function approximators that performed better, as they can adapt more to every kind of relation with the data. <br>\n",
    "\n",
    "Comparing both of the decision trees, as with the regression, we can see similar results in these on both datasets. Therefore, using the reduced dataset is advisable for the same reasons given previously. <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
